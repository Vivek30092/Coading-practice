{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba128bd",
   "metadata": {},
   "source": [
    "># Features of SKlearn\n",
    "Rather than focusing on loading, manipulating and summarising data, Scikit-learn library is focused on modeling the data. Some of the most popular groups of models provided by Sklearn are as follows −\n",
    "\n",
    "`Supervised Learning algorithms` − Almost all the popular supervised learning algorithms, like Linear Regression, Support Vector Machine (SVM), Decision Tree etc., are the part of scikit-learn.\n",
    "\n",
    "`Unsupervised Learning algorithms` − On the other hand, it also has all the popular unsupervised learning algorithms from clustering, factor analysis, PCA (Principal Component Analysis) to unsupervised neural networks.\n",
    "\n",
    "`Clustering` − This model is used for grouping unlabeled data.\n",
    "\n",
    "`Cross Validation` − It is used to check the accuracy of supervised models on unseen data.\n",
    "\n",
    "`Dimensionality Reduction` − It is used for reducing the number of attributes in data which can be further used for summarisation, visualisation and feature selection.\n",
    "\n",
    "`Ensemble methods` − As name suggest, it is used for combining the predictions of multiple supervised models.\n",
    "\n",
    "`Feature extraction` − It is used to extract the features from data to define the attributes in image and text data.\n",
    "\n",
    "`Feature selection` − It is used to identify useful attributes to create supervised models.\n",
    "\n",
    "`Open Source` − It is open source library and also commercially usable under BSD license."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0afa689",
   "metadata": {},
   "source": [
    "## Key words\n",
    "* Dataset - collection of data\n",
    "* Features - variables of data(predictors,inputs or attributes)\n",
    "    - Feature names - list of all the names of the features.\n",
    "    - Feature matrix - the collection of features, in case there are more than one.\n",
    "\n",
    "\n",
    "* Response - output variable, depend on feature variable(target,label or output)\n",
    "    - target names - possible values taken by a response vector\n",
    "    - Response vector - used to represent response column. Generally, we have just one response column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbaa52",
   "metadata": {},
   "source": [
    "## Loading IRIS datset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847562cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      " First 10 rows of X:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"Target names: {target_names}\")\n",
    "print(\"\\n First 10 rows of X:\\n\",X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea420455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "['malignant' 'benign']\n",
      "First 10 rows of X\n",
      "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
      "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
      "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
      "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
      "  3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
      "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
      "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
      "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
      "  6.638e-01 1.730e-01]\n",
      " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
      "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
      "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
      "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
      "  2.364e-01 7.678e-02]\n",
      " [1.245e+01 1.570e+01 8.257e+01 4.771e+02 1.278e-01 1.700e-01 1.578e-01\n",
      "  8.089e-02 2.087e-01 7.613e-02 3.345e-01 8.902e-01 2.217e+00 2.719e+01\n",
      "  7.510e-03 3.345e-02 3.672e-02 1.137e-02 2.165e-02 5.082e-03 1.547e+01\n",
      "  2.375e+01 1.034e+02 7.416e+02 1.791e-01 5.249e-01 5.355e-01 1.741e-01\n",
      "  3.985e-01 1.244e-01]\n",
      " [1.825e+01 1.998e+01 1.196e+02 1.040e+03 9.463e-02 1.090e-01 1.127e-01\n",
      "  7.400e-02 1.794e-01 5.742e-02 4.467e-01 7.732e-01 3.180e+00 5.391e+01\n",
      "  4.314e-03 1.382e-02 2.254e-02 1.039e-02 1.369e-02 2.179e-03 2.288e+01\n",
      "  2.766e+01 1.532e+02 1.606e+03 1.442e-01 2.576e-01 3.784e-01 1.932e-01\n",
      "  3.063e-01 8.368e-02]\n",
      " [1.371e+01 2.083e+01 9.020e+01 5.779e+02 1.189e-01 1.645e-01 9.366e-02\n",
      "  5.985e-02 2.196e-01 7.451e-02 5.835e-01 1.377e+00 3.856e+00 5.096e+01\n",
      "  8.805e-03 3.029e-02 2.488e-02 1.448e-02 1.486e-02 5.412e-03 1.706e+01\n",
      "  2.814e+01 1.106e+02 8.970e+02 1.654e-01 3.682e-01 2.678e-01 1.556e-01\n",
      "  3.196e-01 1.151e-01]\n",
      " [1.300e+01 2.182e+01 8.750e+01 5.198e+02 1.273e-01 1.932e-01 1.859e-01\n",
      "  9.353e-02 2.350e-01 7.389e-02 3.063e-01 1.002e+00 2.406e+00 2.432e+01\n",
      "  5.731e-03 3.502e-02 3.553e-02 1.226e-02 2.143e-02 3.749e-03 1.549e+01\n",
      "  3.073e+01 1.062e+02 7.393e+02 1.703e-01 5.401e-01 5.390e-01 2.060e-01\n",
      "  4.378e-01 1.072e-01]\n",
      " [1.246e+01 2.404e+01 8.397e+01 4.759e+02 1.186e-01 2.396e-01 2.273e-01\n",
      "  8.543e-02 2.030e-01 8.243e-02 2.976e-01 1.599e+00 2.039e+00 2.394e+01\n",
      "  7.149e-03 7.217e-02 7.743e-02 1.432e-02 1.789e-02 1.008e-02 1.509e+01\n",
      "  4.068e+01 9.765e+01 7.114e+02 1.853e-01 1.058e+00 1.105e+00 2.210e-01\n",
      "  4.366e-01 2.075e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dbc = load_breast_cancer()\n",
    "X = dbc.data\n",
    "y = dbc.target\n",
    "print(dbc.feature_names)\n",
    "print(dbc.target_names)\n",
    "print(f\"First 10 rows of X\\n{X[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaa5e3",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "* to check the accuracy of our model\n",
    "* splited into two set\n",
    "    - Training set\n",
    "    - testing set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c270793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4)\n",
      "(45, 4)\n",
      "(105,)\n",
      "(45,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264cf465",
   "metadata": {},
   "source": [
    "<pre>here:\n",
    "- X = feature matrix (collection of features)\n",
    "- y = response vector(use to represent response column,generally we have just one response column)</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541e204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(114, 30)\n",
      "(455,)\n",
      "(114,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dbc = load_breast_cancer()\n",
    "\n",
    "X = dbc.data\n",
    "y = dbc.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a468dea",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "* using KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aaa6766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9833333333333333\n",
      "predictions: [np.str_('setosa'), np.str_('virginica')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data \n",
    "y= iris.target\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "Classifier_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "Classifier_knn.fit(X_train,y_train)\n",
    "y_pred= Classifier_knn.predict(X_test)\n",
    "# Finding accuracy by comparing actual response values(y_test)with predicted response value(y_pred)\n",
    "print(\"Accuracy: \",metrics.accuracy_score(y_test,y_pred))\n",
    "# providing sample data and the model will make prediction out of that data\n",
    "\n",
    "sample = [[5,5,3,2],[2,4,3,5]]\n",
    "preds = Classifier_knn.predict(sample)\n",
    "pred_species = [iris.target_names[p] for p in preds] \n",
    "print(\"predictions:\",pred_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf8d504",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "* model should be persist for future use so that we do not need to retrain it again and again. It can be done with the help of `dump` and `load` features of `joblib` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f752a",
   "metadata": {},
   "source": [
    "<pre>\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier_knn, 'iris_classifier_knn.joblib')\n",
    "</pre>\n",
    "\n",
    "* iris_classifier_knn.joblib is file name for saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211bdb09",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "* converting data into meaningful data\n",
    "* sklearn has package `preprocessing` for it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74fe4ca",
   "metadata": {},
   "source": [
    "> Binarisation\n",
    "* to convert our numerical values into boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b158192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binarized data:\n",
      " [[1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "input_data = np.array(\n",
    "   [[2.1, -1.9, 5.5],\n",
    "   [-1.5, 2.4, 3.5],\n",
    "   [0.5, -7.9, 5.6],\n",
    "   [5.9, 2.3, -5.8]]\n",
    ")\n",
    "data_binarized = preprocessing.Binarizer(threshold=0.5).transform(input_data)\n",
    "print(\"\\nBinarized data:\\n\", data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f19e7b",
   "metadata": {},
   "source": [
    "In the above example, we used `threshold value` = 0.5 and that is why, all the values above 0.5 would be converted to 1, and all the values below 0.5 would be converted to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e714095",
   "metadata": {},
   "source": [
    "> Mean Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79966504",
   "metadata": {},
   "source": [
    "* This technique is used to eliminate the mean from feature vector so that every feature centered on zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "935ecba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = [ 1.75  -1.275  2.2  ]\n",
      "Stddeviation =  [2.71431391 4.20022321 4.69414529]\n",
      "Mean_removed = [1.11022302e-16 0.00000000e+00 0.00000000e+00]\n",
      "Stddeviation_removed = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "Input_data = np.array(\n",
    "   [[2.1, -1.9, 5.5],\n",
    "   [-1.5, 2.4, 3.5],\n",
    "   [0.5, -7.9, 5.6],\n",
    "   [5.9, 2.3, -5.8]]\n",
    ")\n",
    "\n",
    "#displaying the mean and the standard deviation of the input data\n",
    "print(\"Mean =\", input_data.mean(axis=0))\n",
    "print(\"Stddeviation = \", input_data.std(axis=0))\n",
    "#Removing the mean and the standard deviation of the input data\n",
    "\n",
    "data_scaled = preprocessing.scale(input_data)\n",
    "print(\"Mean_removed =\", data_scaled.mean(axis=0))\n",
    "print(\"Stddeviation_removed =\", data_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3d424",
   "metadata": {},
   "source": [
    "> Scaling\n",
    "* We use this preprocessing technique for scaling the feature vectors. Scaling of feature vectors is important, because the features should not be synthetically large or small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c66357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min max scaled data:\n",
      " [[0.48648649 0.58252427 0.99122807]\n",
      " [0.         1.         0.81578947]\n",
      " [0.27027027 0.         1.        ]\n",
      " [1.         0.99029126 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "Input_data = np.array(\n",
    "   [\n",
    "      [2.1, -1.9, 5.5],\n",
    "      [-1.5, 2.4, 3.5],\n",
    "      [0.5, -7.9, 5.6],\n",
    "      [5.9, 2.3, -5.8]\n",
    "   ]\n",
    ")\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(input_data)\n",
    "print (\"\\nMin max scaled data:\\n\", data_scaled_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb3d91",
   "metadata": {},
   "source": [
    "> Normalisation\n",
    "* used to modifying the feature vectors\n",
    "* necessary so that the feature vectors can be measured at common scale.\n",
    "* it's of two type\n",
    "    - L1 Normalisation\n",
    "    - L2 Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f566728",
   "metadata": {},
   "source": [
    ">> L1 Normalisation\n",
    "* also called Least Absolute Deviations.\n",
    "* It modifies the value in such a manner that the sum of the absolute values remains always up to 1 in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec9f306c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 normalized data:\n",
      " [[ 0.22105263 -0.2         0.57894737]\n",
      " [-0.2027027   0.32432432  0.47297297]\n",
      " [ 0.03571429 -0.56428571  0.4       ]\n",
      " [ 0.42142857  0.16428571 -0.41428571]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "Input_data = np.array(\n",
    "   [\n",
    "      [2.1, -1.9, 5.5],\n",
    "      [-1.5, 2.4, 3.5],\n",
    "      [0.5, -7.9, 5.6],\n",
    "      [5.9, 2.3, -5.8]\n",
    "   ]\n",
    ")\n",
    "data_normalized_l1 = preprocessing.normalize(input_data, norm='l1')\n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6888c0",
   "metadata": {},
   "source": [
    ">> L2 Normalisation\n",
    "* Also called Least Squares.\n",
    "* modifies the value in such a manner that the sum of the squares remains always up to 1 in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d95fb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 normalized data:\n",
      " [[ 0.33946114 -0.30713151  0.88906489]\n",
      " [-0.33325106  0.53320169  0.7775858 ]\n",
      " [ 0.05156558 -0.81473612  0.57753446]\n",
      " [ 0.68706914  0.26784051 -0.6754239 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "Input_data = np.array(\n",
    "   [\n",
    "      [2.1, -1.9, 5.5],\n",
    "      [-1.5, 2.4, 3.5],\n",
    "      [0.5, -7.9, 5.6],\n",
    "      [5.9, 2.3, -5.8]\n",
    "   ]\n",
    ")\n",
    "data_normalized_l2 = preprocessing.normalize(input_data, norm='l2')\n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
